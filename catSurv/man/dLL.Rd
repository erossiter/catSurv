% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{dLL}
\alias{dLL}
\title{The first derivative of the log-likelihood}
\usage{
dLL(cat_df, theta, use_prior)
}
\arguments{
\item{cat_df}{An object of \code{Cat} class}

\item{theta}{A double indicating the potential value for \eqn{\theta_j}}

\item{use_prior}{A logical indicating whether to use the prior parameters in estimation}
}
\value{
A value indicating the derivative of the log-likelihood (or log-posterior) for a respondent's answer profile.
}
\description{
When \code{usePrior = FALSE}, this function evaluates the first derivative of the log-likelihood evaluated at point \eqn{\theta}.  
When \code{usePrior = TRUE}, this function evaluates the first derivative of the log-posterior evaluated at point \eqn{\theta}.
}
\details{
For the dichotomous case, \eqn{P_{ij}} is the probability that person \eqn{j} will answer question \eqn{i} correctly conditioned on their ability parameter \eqn{\theta_j}.  
  Let \eqn{\mu_\theta} be the prior mean and \eqn{\sigma_\theta} be the prior standard deviation.  
  Further, let \eqn{Q_{ij} = 1 - P_{ij}}. Using this notation, the first derivative of the log-likelihood is given by:

  \deqn{L_\theta = \sum_{i=1}^n b_i\Big(\frac{P_{ij} - c_i}{P_{ij}(1-c_i)} \Big)(y_{ij} - P_{ij}) }{L_\theta = \sum_{i = 1}^n b_i ((P_ij - c_i)/(P_ij (1 - c_i)))(y_ij - P_ij)} 

  The first derivative of the log-posterior is:

  \deqn{L_\theta = \sum_{i=1}^n \Big [ b_i\Big(\frac{P_{ij} - c_i}{P_{ij}(1-c_i)} \Big)(y_{ij} - P_{ij}) \Big ] -  \Big(\frac{\theta_j - \mu_\theta}{\sigma^2_\theta} \Big)}{L_\theta = \sum_{i = 1}^n [ b_i ((P_ij - c_i)/(P_ij (1 - c_i)))(y_ij - P_ij)] - ((\theta_j - \mu_\theta)(\sigma^2_\theta))}

  For the polytomous case, \deqn{P_{ijk} = P^*_{ij,k} - P^*_{ij,k-1}}{P_ijk = P*_{ij,k} - P*_{ij,k-1}} and \deqn{P^*_{ijk} = Pr(y_{ij}<k|\theta_j)}{P*_{ijk} = Pr(y_{ij}<k| \theta_j)} and \deqn{Q_{ijk}^*= 1- P^*_{ijk}}{Q*_ijk = 1 - P*_ijk} The log-likelihood is then given by:

  \deqn{L = \sum^n_{i=1}\sum^{m_i}_{k=1}I(y_{ijk}=k)\log P_{ijk}}{L = \sum^n_{i = 1} \sum^{m_i}_{k = 1} I(y_ijk = k) log P_ijk},

  where \deqn{I(\cdot)}{I(.)} is the usual indicator function that evaluates to 1 when the condition is met and 0 otherwise.  
  In order to calculate the estimate of \eqn{\theta}, we need both the first and the second derivatives of the log-likelihood function with respect to \eqn{\theta_j}. Therefore, the first derivative of \eqn{L} with respect to \eqn{\theta_j} is:

  \deqn{\frac{\partial L}{\partial \theta_j} &= \sum_{i=1}^n\sum^{m_i}_{k=1} I(y_{ijk}=k)\Big[- \beta_i \Big ( \frac{w_{ik}-w_{i,k-1}}{P_{ik}} \Big )\Big]}{{\partial L}/{\partial \theta_j} = \sum_{i = 1}^n \sum^{m_i}_{k = 1} I(y_ijk = k)[-\beta_i ((w_{ik} - w_{i,k-1})/(P_ik))]}

  where \deqn{w_{i,k-1} = P^*_{i,k-1}Q^*_{i,k-1}$ and $w_{ik}=P^*_{ik}Q^*_{ik}}{w_{i,k-1} = P*_{i,k-1} Q*_{i,k-1}} and \eqn{w_{ik} = P*_{ik} Q*_{ik}}.

  The log posterior, however, is:

  \deqn{\frac{\partial L}{\partial \theta_j} &= \sum_{i=1}^n\sum^{m_i}_{k=1} I(y_{ijk}=k)\Big[- \beta_i \Big ( \frac{w_{ik}-w_{i,k-1}}{P_{ik}} \Big ) \Big] - \Big(\frac{\theta_j - \mu_\theta}{\sigma^2_\theta} \Big) \Big]}{{\partial L}/{\partial \theta_j} = \sum_{i = 1}^n \sum^{m_i}_{k = 1} I(y_ijk = k)[-\beta_i ((w_{ik} - w_{i,k-1})/(P_{ik})) - ((\theta_j - \mu_\theta)/(\sigma^2_\theta))]}

Note: These method will only be available using the normal prior distribution
}

